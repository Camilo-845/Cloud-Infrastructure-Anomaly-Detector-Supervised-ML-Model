{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc525dd",
   "metadata": {},
   "source": [
    "# Modelo ML Supervisado detector Anomalias en intraestructura en la nube\n",
    "\n",
    "# I. Descripción del problema e inspección del conjunto de datos\n",
    "\n",
    "## 1. Descripción del problema y conjuto de datos\n",
    "\n",
    "El objetivo es identificar automáticamente, basándose en un conjunto de métricas operacionales (como el uso de CPU, memoria, tráfico de red, y consumo de energía), cuándo una VM se está comportando de manera anómala o inusual, en lugar de su patrón de comportamiento normal.\n",
    "\n",
    "**Finalidad**: Este tipo de detección es crucial para la monitorización del rendimiento, el mantenimiento predictivo, y la fiabilidad general de los servicios en la nube, permitiendo identificar fallos de hardware, software o posibles incidentes de seguridad de forma temprana.\n",
    "\n",
    "**Aproximación**: El dataset proporciona una etiqueta binaria (\"Anomaly status\") para cada conjunto de métricas, lo que permite el desarrollo y la evaluación de modelos de Machine Learning supervisado para clasificar si un estado de la VM es normal o anómalo\n",
    "\n",
    "El conjunto de datos proviene de un dataset de Kanggle https://www.kaggle.com/datasets/sandhyapeesara/cloud-anomaly-data\n",
    "\n",
    "### Descripción de Variables\n",
    "\n",
    "| Variable | Tipo de Dato (Implícito) | Descripción Mejorada |\n",
    "| :--- | :--- | :--- |\n",
    "| **vm\\_id** | Categórico / ID | **Identificador único** de la Máquina Virtual (VM) en el entorno de la nube. Permite rastrear el rendimiento individual de cada instancia. |\n",
    "| **timestamp** | Temporal | **Marca de tiempo** exacta de la medición. Es fundamental para el análisis de **series de tiempo** y la detección de patrones temporales o estacionalidad. |\n",
    "| **cpu\\_usage** | Numérico (Porcentaje) | **Utilización del Procesador Central (CPU)**, expresada como un porcentaje (0-100%). Mide la carga de trabajo de la VM. |\n",
    "| **memory\\_usage** | Numérico (Porcentaje) | **Utilización de la Memoria RAM**, expresada como un porcentaje (0-100%). Indica la cantidad de memoria activa consumida por los procesos. |\n",
    "| **network\\_traffic** | Numérico | **Volumen de Tráfico de Red** (entrada/salida) de la VM. Medido en una escala relativa (0-1000); podría representar bytes/segundo o paquetes/segundo. |\n",
    "| **power\\_consumption** | Numérico | **Consumo de Energía** de la VM en la unidad de tiempo. Medido en una escala relativa (0-500); la unidad física podría ser Vatios (W). |\n",
    "| **num\\_executed\\_instructions** | Numérico (Contador) | **Número de Instrucciones** ejecutadas por la CPU durante el intervalo de medición. Mide la productividad computacional. |\n",
    "| **execution\\_time** | Numérico | **Tiempo de Ejecución** de las tareas o procesos en la VM. Medido en una escala relativa (0-100); podría representar milisegundos o un tiempo de respuesta normalizado. |\n",
    "| **energy\\_efficiency** | Numérico (Ratio) | **Eficiencia Energética**. Un valor normalizado entre 0 y 1, que generalmente se calcula como una razón entre el rendimiento (ej. instrucciones ejecutadas) y el consumo de energía. |\n",
    "| **task\\_type** | Categórico | **Tipo de Tarea** principal que se está ejecutando en la VM en ese momento. Categorías clave: **'io'** (Intensiva en Entrada/Salida), **'network'** (Intensiva en Red), **'other'**. |\n",
    "| **task\\_priority** | Categórico Ordinal | **Nivel de Prioridad** asignado a la tarea en curso. Categorías: **'low'** (Baja), **'high'** (Alta), **'other'** (Otras o media/variable). |\n",
    "| **task\\_status** | Categórico | **Estado Actual de la Tarea** medida. Categorías: **'waiting'** (En espera de recursos), **'running'** (Actualmente en ejecución), **'other'** (Otros estados, como completada o fallida). |\n",
    "| **Anomaly status** | Binario (Target) | **Estado de Anomalía**. Esta es la **variable objetivo** (`target`). Un valor de **1** indica que el comportamiento de la VM se considera **Anómalo**, y **0** indica un comportamiento **Normal**. |\n",
    "\n",
    "### Contenido Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d5911",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e12af6",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19af673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"Cloud_Anomaly_Dataset.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"sandhyapeesara/cloud-anomaly-data\",\n",
    "  file_path,\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569c83d",
   "metadata": {},
   "source": [
    "#### Resumen Estadistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2c445",
   "metadata": {},
   "source": [
    "### Identificación de datos unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30a56b",
   "metadata": {},
   "source": [
    "Preliminarmente no se puede descartar ningura variable por ser constate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05245ac",
   "metadata": {},
   "source": [
    "#### Identificación de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0524bc9",
   "metadata": {},
   "source": [
    "Se debe tener el cuenta las variables 'cpu_usage', 'memory_usage', 'network_traffic', 'power_comsuption', 'num_excecuted_instrucitons', 'excecution_time', energy_efficiency','task_type', 'trask_priority'y task_status' que cuentan datos nulos, donde se debe aplicacar una estrategía de corrección de los mismos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e03c8",
   "metadata": {},
   "source": [
    "### Histogramas y diagramas de cajas para valores numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numéricas para análisis\n",
    "numerical_vars = ['cpu_usage', 'memory_usage', 'network_traffic', 'power_consumption', 'num_executed_instructions',\n",
    "'execution_time', 'energy_efficiency']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbef5f0",
   "metadata": {},
   "source": [
    "\n",
    "#### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generar histogramas\n",
    "df_numeric = df.select_dtypes(include=['float64'])\n",
    "\n",
    "df_numeric.hist(edgecolor='black', linewidth=1.2, bins=30)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.suptitle('Distribución de variables numéricas', fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f01b8e",
   "metadata": {},
   "source": [
    "Basado en los diagramas generados, podemos observar que todos siguen una distribución uniforme. Esto significa que todos los valores dentro del rando de la variable tienen aproximadamente la misma probabilidad de ocurrir. Lo que implica analisar las variables desde otro punto de vista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ca601",
   "metadata": {},
   "source": [
    "#### Diagramas de cajas y bigotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_numeric = df.select_dtypes(include=['float64'])\n",
    "numerical_vars = df_numeric.columns\n",
    "n_vars = len(numerical_vars)\n",
    "\n",
    "n_cols = 3 \n",
    "n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x=df[var], ax=ax)\n",
    "    ax.set_title(f'{var}')\n",
    "    ax.set_xlabel('') # Opcional: limpia la etiqueta x para no ser redundante\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Diagramas de cajas y bigotes', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4458c5",
   "metadata": {},
   "source": [
    "Podemos observar que no hay puntos fuera de los bigotes, que los bigotes lleguen a los extremas es concecuencia de la distribución uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe0dba",
   "metadata": {},
   "source": [
    "Aunque la distribuición total parezca uniforme, es muy probable que las distribuciones para los datos normales y anómalos sean diferentes entre sí.\n",
    "\n",
    "Por tanto deberiamos visualizar por Categoría de Anomalía"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ac86c",
   "metadata": {},
   "source": [
    "### Visualizar por Categoría de Anomalía\n",
    "Generar los gráficos, pero esta vez, separando los datos por el valor de la columna **Anomaly status**. Esto nos permitirá comparar la distribucón de los casos normales vs los casos anómalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['float64'])\n",
    "numerical_vars = df_numeric.columns\n",
    "n_vars = len(numerical_vars)\n",
    "\n",
    "n_cols = 3 \n",
    "n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    ax = axes[i]\n",
    "    sns.histplot(data=df, x=var, hue='Anomaly status', kde=True, palette='viridis', ax=ax)\n",
    "    ax.set_title(f'Distribución de {var}')\n",
    "    ax.set_xlabel('') # Opcional: limpia la etiqueta x para no ser redundante\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Distribución de variables numéricas (Normal vs. Anómalo)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63221c9c",
   "metadata": {},
   "source": [
    "#### Análisis de Distribuciones por Estado de Anomalía\n",
    "\n",
    "Al visualizar las distribuciones de las variables numéricas segmentadas por la variable objetivo (Anomaly status), se\n",
    "revelan patrones cruciales para la construcción del modelo:\n",
    "\n",
    "* Desbalance de Clases Significativo: Se confirma una característica común en problemas de detección de anomalías: un\n",
    "    fuerte desbalance de clases. El conjunto de datos contiene una proporción considerablemente mayor de instancias\n",
    "    normales (0) en comparación con las anómalas (1). Este factor es crítico y deberá ser gestionado antes de la fase de\n",
    "    entrenamiento mediante balanceo\n",
    "\n",
    "* Patrón en `network_traffic`: Esta variable demuestra ser un fuerte indicador de anomalías. Se observa una alta\n",
    "    concentración de instancias anómalas en el rango de valores altos (aproximadamente 950-1000). Por el contrario, las\n",
    "    instancias normales son muy infrecuentes en este mismo rango, lo que sugiere que un tráfico de red excepcionalmente\n",
    "    alto es un síntoma claro de un comportamiento anómalo.\n",
    "\n",
    "* Patrón en `energy_efficiency`: De manera similar, la eficiencia energética es un predictor clave. Los valores bajos de\n",
    "    energy_efficiency (en el intervalo de 0.0 a 0.3) están fuertemente correlacionados con la ocurrencia de anomalías. Las\n",
    "    instancias normales, en cambio, tienden a presentar una eficiencia energética más alta y muestran una frecuencia muy\n",
    "    baja en este rango.\n",
    "\n",
    "#### Conclusión del Análisis Visual:\n",
    "\n",
    "El análisis exploratorio revela que, aunque las distribuciones generales de las variables pueden parecer uniformes,\n",
    "existen diferencias claras y medibles en las distribuciones cuando se separan por clase (normal vs. anómela). Variables\n",
    "como network_traffic y energy_efficiency son especialmente prometedoras y probablemente tendrán un alto poder predictivo\n",
    "en los modelos de machine learning que se desarrollarán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e905cde",
   "metadata": {},
   "source": [
    "### Visualizacón de Variables Categóricas\n",
    "Para entender si las difenretes categorías se relacionan con el estado de anomalía\n",
    "\n",
    "Para las variables categóricas como `task_type`, `task_priority` y `task_status`, los diagramas de barras son la herramienta ideal. Nos permitiran ver la frecuancia absoluta o relativa de cada categoría\n",
    "\n",
    "Al igual que el analisis previo, es fundamental analizar estas distrivuciones separados por el `anomaly_status` para identificar si ciertas categorías están más asociadas con el comportamietno anómalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las variables categóricas a visualizar\n",
    "categorical_vars = ['task_type', 'task_priority', 'task_status']\n",
    "\n",
    "# Generar diagramas de barras para cada variable categórica, separados por Anomaly status\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=var, hue='Anomaly status', palette='viridis')\n",
    "    plt.title(f'Frecuencia de {var} por Estado de Anomalía')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Conteo')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064313e6",
   "metadata": {},
   "source": [
    "- Análisis de `task_status`:\n",
    "    - En el caso de las anomalías, se observa una frecuencia ligeramente superior de instancias en el estado completed en\n",
    "       comparación con los estados running y waiting, los cuales presentan frecuencias muy similares entre sí.\n",
    "    - Para las instancias normales, la categoría completed exhibe una frecuencia marginalmente inferior a las otras dos\n",
    "      categorías (running y waiting).\n",
    "    - Implicación: Este patrón sugiere que, aunque el estado completed podría parecer benigno, una proporción\n",
    "      relativamente mayor de anomalías se manifiesta en este estado. Esto indica que el task_status podría ser un factor\n",
    "      discriminatorio, aunque sutil, para la detección de anomalías, y su interacción con otras variables podría ser\n",
    "      relevante.\n",
    "\n",
    "- Análisis de `task_type` y `task_priority`: Para estas variables, la distribución de frecuencias de sus categorías es\n",
    "    similar tanto para las instancias normales como para las anómalas, manteniendo la proporción general de desbalance de\n",
    "    clases observada en el dataset. Esto sugiere que, de forma aislada, task_type y task_priority no presentan un poder\n",
    "    discriminatorio directo significativo para identificar anomalías. Sin embargo, podrían ser relevantes en combinación\n",
    "    con otras características o para contextualizar los eventos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fae4d2",
   "metadata": {},
   "source": [
    "### Analisis de correlación\n",
    "El análisis de correlación es un paso crucial para entender las relaciones entre las variables numéricas, y es fundamental para la selección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12731384",
   "metadata": {},
   "source": [
    "#### Analisis de correlación con Mapa de Calor\n",
    "\n",
    "Una Matriz de correlación nos mmostrarás el coeficiente de correlación (Pearson) entre cada par de variables númericas. Un mapa de calor es una excelente manera de visualizar esta matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars_for_corr = [\n",
    "    'cpu_usage',\n",
    "    'memory_usage',\n",
    "    'network_traffic',\n",
    "    'power_consumption',\n",
    "    'num_executed_instructions',\n",
    "    'execution_time',\n",
    "    'energy_efficiency',\n",
    "    'Anomaly status' # Incluimos la variable objetivo\n",
    "]\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = df[numerical_vars_for_corr].corr()\n",
    "\n",
    "# Generar el mapa de calor\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Matriz de Correlación de Variables Numéricas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523ba61",
   "metadata": {},
   "source": [
    "#### Análisis de Correlación\n",
    "\n",
    "El análisis del mapa de calor revela los siguientes puntos clave:\n",
    "\n",
    "- Principal Predictor Lineal (`energy_efficiency`): La variable energy_efficiency muestra la correlación más\n",
    "    significativa con Anomaly status, con un coeficiente de -0.22. Esta correlación negativa moderada indica que una menor\n",
    "    eficiencia energética está linealmente asociada con una mayor probabilidad de anomalía. Este resultado cuantitativo\n",
    "    refuerza los hallazgos del análisis visual, donde se observó una alta frecuencia de anomalías en los rangos bajos de\n",
    "    esta variable.\n",
    "\n",
    "- Correlación Débil pero Relevante (`network_traffic`): La variable network_traffic presenta una correlación positiva\n",
    "    débil de 0.04 con la variable objetivo. Aunque el valor es bajo, es el segundo más relevante y sugiere que un aumento\n",
    "    en el tráfico de red tiene una ligera asociación lineal con la ocurrencia de anomalías, lo cual también es consistente\n",
    "    con el análisis visual previo.\n",
    "\n",
    "- Baja Correlación de Otras Variables: Las demás variables numéricas (cpu_usage, memory_usage, power_consumption, etc.)\n",
    "    exhiben coeficientes de correlación muy cercanos a cero con Anomaly status. Esto implica que no poseen una relación\n",
    "    lineal fuerte con la variable objetivo de forma individual.\n",
    "\n",
    "- Ausencia de Multicolinealidad: Se observa una correlación extremadamente baja entre las variables predictoras. La\n",
    "    ausencia de multicolinealidad es beneficiosa, ya que indica que las características son en gran medida independientes y\n",
    "    no aportan información redundante, lo que simplifica la interpretación del modelo.\n",
    "\n",
    "#### Conclusión del Análisis de Correlación:\n",
    "\n",
    "El análisis de correlación posiciona a energy_efficiency y, en menor medida, a network_traffic como las características\n",
    "numéricas con mayor poder predictivo lineal. Estos hallazgos serán fundamentales para la etapa de selección de\n",
    "características y la construcción del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48151155",
   "metadata": {},
   "source": [
    "# II. Diseño de Experimentos y Recolección de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee16c7",
   "metadata": {},
   "source": [
    "## 3. Estrategia de Preparación de Datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c848e",
   "metadata": {},
   "source": [
    "#### Corrigiendo datos faltantes\n",
    "\n",
    "La estrategía que consideramos para rellenar los datos faltantes es por la media, ya que en analisis previo observamos un comportameindo uniforme, la mediana es una medida robusta para este caso, ya que no se va a ver afectada por valores extremos\n",
    "\n",
    "Para las variables categóricas (task_type, task_priority, task_status), no podemos calcular una media o mediana. La estrategia\n",
    "estándar y más lógica es usar la moda, que es el valor que aparece con más frecuencia en la columna. De esta manera, asignamos la\n",
    "categoría más probable al dato faltante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación de valores nulos para variables numéricas con la mediana\n",
    "numerical_cols_with_na = ['cpu_usage', 'memory_usage', 'network_traffic', 'power_consumption', 'num_executed_instructions',\n",
    "'execution_time', 'energy_efficiency']\n",
    "\n",
    "for col in numerical_cols_with_na:\n",
    "    median_value = df[col].median()\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "\n",
    "# Imputación de valores nulos para variables categóricas con la moda\n",
    "categorical_cols_with_na = ['task_type', 'task_priority', 'task_status']\n",
    "\n",
    "for col in categorical_cols_with_na:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_value)\n",
    "\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49009a03",
   "metadata": {},
   "source": [
    "#### Codificación de variables categoricas\n",
    "Los modelos de ML no pueden trabajar directamente con texto (como 'low', 'high', 'io'). Necesitamos convertir todas las variables a formato numérico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e2e98",
   "metadata": {},
   "source": [
    "\n",
    "Tenemos dos tipos de variables categóricas que requieren un tratamiento diferente:\n",
    "\n",
    "A. Variables Nominales (`task_type`, `task_status`)\n",
    "Estas no tienen un orden inherente. Usaremos One-Hot Encoding, que crea nuevas columnas binarias (0 o 1) para cada categoría.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['task_type', 'task_status'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8553c",
   "metadata": {},
   "source": [
    "B. Variable Ordinal (`task_priority`)\n",
    "Esta variable sí tiene un orden ('low' < 'other' < 'high'). Aquí, asignaremos un número a cada categoría para preservar esa jerarquía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el orden y mapear\n",
    "priority_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "df['task_priority'] = df['task_priority'].map(priority_mapping)\n",
    "print(df['task_priority'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2242fc2",
   "metadata": {},
   "source": [
    "#### Separar nuestras variable objetivo y variables caracteristicas\n",
    "\n",
    "Separaremos nuestro DataFrame en la variable objetivo (y) que en nuestro caso es `Anomaly status` y las caracteristicas (x) que usaremos para la predicción. También eliminaremos las columnas que no son útiles para el modelo, como `vm_id` y `timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'y' es nuestra variable objetivo\n",
    "y = df['Anomaly status']\n",
    "\n",
    "# 'X' son todas las demás columnas, excepto las que no aportan información predictiva\n",
    "X = df.drop(columns=['Anomaly status', 'vm_id', 'timestamp'])\n",
    "\n",
    "print(\"Dimensiones de X:\", X.shape)\n",
    "print(\"Dimensiones de y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0147b",
   "metadata": {},
   "source": [
    "#### División en Conjuntos de Entrenamiento y Prueba\n",
    "\n",
    "Dividimos los datos en un conjunto para entrenar el modelo y otro para evaluarlo de forma imaparcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e78743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir los datos (70% entrenamiento, 30% prueba)\n",
    "# stratify=y asegura que la proporción de anomalías sea la misma en ambos conjuntos, lo cual es clave en datasets desbalanceados.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Tamaño de X_train:\", X_train.shape)\n",
    "print(\"Tamaño de X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfc8bd",
   "metadata": {},
   "source": [
    "#### Balanceo de clases\n",
    "\n",
    "Ya que hemos dividido los datos correctamente, podemos abordar el problema del desbalance\n",
    "\n",
    "No solo debemos dubplicar los casos de anomalias, sino que cree nuevas muestras sintéticas que sean similares a las existentes, para este proposito usaremos la tecnica **SMOTE SMOTE (Synthetic Minority Over-sampling Technique)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Aplicar SMOTE SOLO a los datos de entrenamiento\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=y_train, hue=y_train, ax=axes[0], palette='viridis', legend=False)\n",
    "axes[0].set_title('Antes de SMOTE (Datos de Entrenamiento)')\n",
    "axes[0].set_xlabel('Estado de Anomalía')\n",
    "axes[0].set_ylabel('Conteo')\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "sns.countplot(x=y_train_resampled, hue=y_train_resampled, ax=axes[1], palette='viridis', legend=False)\n",
    "axes[1].set_title('Después de SMOTE (Datos de Entrenamiento)')\n",
    "axes[1].set_xlabel('Estado de Anomalía')\n",
    "axes[1].set_ylabel('Conteo')\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41821e3f",
   "metadata": {},
   "source": [
    "Los datos ya se encuentra balanceados correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbfaa6",
   "metadata": {},
   "source": [
    "#### Selección de Características\n",
    "\n",
    "Después de preparar y balancear los datos, el siguiente paso es seleccionar las características más influyentes para predecir el\n",
    "estado de anomalía. Usaremos métodos estadísticos para evaluar la relación de cada característica con la variable objetivo (`Anomaly\n",
    "status`).\n",
    "\n",
    "- Para las **variables numéricas**, usaremos la prueba **ANOVA F-test** (`f_classif`), que nos dirá si hay una diferencia\n",
    "significativa en las medias de la característica entre las clases \"Normal\" y \"Anómala\".\n",
    "- Para las **variables categóricas** (ya codificadas), usaremos la prueba **Chi-Cuadrado** (`chi2`), que evalúa la dependencia entre\n",
    "la característica y la clase objetivo.\n",
    "\n",
    "Esto nos ayudará a reducir la dimensionalidad, mejorar el rendimiento del modelo y reducir el riesgo de sobreajuste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0404ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las columnas numéricas y categóricas en nuestro conjunto de datos procesado\n",
    "# 'task_priority' se trata como numérica porque la mapeamos a números (0, 1, 2)\n",
    "numerical_features = [\n",
    "    'cpu_usage', 'memory_usage', 'network_traffic', 'power_consumption',\n",
    "    'num_executed_instructions', 'execution_time', 'energy_efficiency', 'task_priority'\n",
    "]\n",
    "\n",
    "# Las columnas categóricas son las que se crearon con get_dummies()\n",
    "categorical_features = [\n",
    "    'task_type_io', 'task_type_network',\n",
    "    'task_status_running', 'task_status_waiting'\n",
    "]\n",
    "\n",
    "# Es posible que los nombres de las columnas categóricas varíen ligeramente.\n",
    "# Vamos a verificar y corregir los nombres si es necesario.\n",
    "# El siguiente código asegura que solo usemos las columnas que realmente existen en nuestro DataFrame.\n",
    "\n",
    "existing_categorical_features = [col for col in categorical_features if col in X_train_resampled.columns]\n",
    "\n",
    "# Si get_dummies() usó 'other' o 'completed' como base, los nombres de las columnas serían diferentes.\n",
    "# Vamos a listar todas las columnas dummies para estar seguros.\n",
    "print(\"Columnas del DataFrame:\", X_train_resampled.columns.tolist())\n",
    "print(\"-\" * 30)\n",
    "print(\"Columnas categóricas identificadas:\", existing_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64551b",
   "metadata": {},
   "source": [
    "Aplicamos ANOVA F-test a las carecteristicas númericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configurar el selector de características para usar f_classif\n",
    "# Seleccionaremos todas las características (k='all') para ver la puntuación de cada una\n",
    "fs_numerical = SelectKBest(score_func=f_classif, k='all')\n",
    "\n",
    "# 2. Aplicar el selector a nuestras variables numéricas del conjunto de entrenamiento\n",
    "fs_numerical.fit(X_train_resampled[numerical_features], y_train_resampled)\n",
    "\n",
    "# 3. Crear un DataFrame con las puntuaciones para una fácil visualización\n",
    "numerical_scores = pd.DataFrame({\n",
    "    'Feature': numerical_features,\n",
    "    'F-Score': fs_numerical.scores_,\n",
    "    'P-Value': fs_numerical.pvalues_\n",
    "}).sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "print(\"Puntuaciones de Características Numéricas (ANOVA F-test):\")\n",
    "print(numerical_scores)\n",
    "\n",
    "# 4. Visualizar las puntuaciones\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='F-Score', y='Feature', data=numerical_scores, palette='viridis')\n",
    "plt.title('Importancia de Características Numéricas (F-Score)')\n",
    "plt.xlabel('Puntuación F (ANOVA)')\n",
    "plt.ylabel('Característica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63d6a9",
   "metadata": {},
   "source": [
    "Aplicar la prueba de Chi-cuadrado a las caracteristicas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbde898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar la lista de características categóricas con los nombres confirmados\n",
    "categorical_features = ['task_type_io', 'task_type_network', 'task_status_running', 'task_status_waiting']\n",
    "\n",
    "# 1. Configurar el selector de características para usar chi2\n",
    "# Seleccionaremos todas las características (k='all') para ver la puntuación de cada una\n",
    "fs_categorical = SelectKBest(score_func=chi2, k='all')\n",
    "\n",
    "# 2. Aplicar el selector a nuestras variables categóricas del conjunto de entrenamiento\n",
    "fs_categorical.fit(X_train_resampled[categorical_features], y_train_resampled)\n",
    "\n",
    "# 3. Crear un DataFrame con las puntuaciones para una fácil visualización\n",
    "categorical_scores = pd.DataFrame({\n",
    "    'Feature': categorical_features,\n",
    "    'Chi2-Score': fs_categorical.scores_,\n",
    "    'P-Value': fs_categorical.pvalues_\n",
    "}).sort_values(by='Chi2-Score', ascending=False)\n",
    "\n",
    "print(\"\\nPuntuaciones de Características Categóricas (Chi-Cuadrado):\")\n",
    "print(categorical_scores)\n",
    "\n",
    "# 4. Visualizar las puntuaciones\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Chi2-Score', y='Feature', data=categorical_scores, palette='magma')\n",
    "plt.title('Importancia de Características Categóricas (Chi-Cuadrado Score)')\n",
    "plt.xlabel('Chi2 Score')\n",
    "plt.ylabel('Característica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa15e31",
   "metadata": {},
   "source": [
    "#### Análisis de los Resultados\n",
    "\n",
    "1. Características Numéricas (ANOVA F-test):\n",
    "\n",
    " - Muy Importantes: energy_efficiency, task_priority, y network_traffic tienen puntuaciones F-Score altísimas. Esto confirma lo que ya\n",
    "   sospechábamos en el análisis visual: son, por mucho, los predictores numéricos más fuertes.\n",
    " - Medianamente Importantes: memory_usage también es claramente relevante, aunque en menor medida.\n",
    " - Poco Importantes: execution_time y power_consumption tienen una relación estadísticamente significativa (P-Value < 0.05), pero su\n",
    "   puntuación F es muy baja, lo que sugiere que su poder predictivo es débil.\n",
    " - No Relevantes: num_executed_instructions y cpu_usage tienen un P-Value > 0.05. Esto significa que no podemos descartar que su\n",
    "   relación con la anomalía sea producto del azar. Por lo tanto, son candidatos claros a ser eliminados.\n",
    "\n",
    "2. Características Categóricas (Chi-Cuadrado):\n",
    "\n",
    " - Todas las características categóricas (task_type_io, task_type_network, task_status_running, task_status_waiting) muestran\n",
    "   puntuaciones Chi2-Score muy altas y P-Values de 0.0.\n",
    " - Esto indica que todas ellas tienen una fuerte asociación con la variable objetivo y son importantes para el modelo. Las relacionadas\n",
    "   con el tipo de tarea (task_type) parecen ser ligeramente más fuertes que las del estado (task_status), pero todas son muy valiosas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3cd8b",
   "metadata": {},
   "source": [
    "Basado en estos resultados vamos a:\n",
    "\n",
    "- Conservar todas las características categoricas\n",
    "- Conservar las 6 caracteristicas númericas que mostraron una relación estadisticamente significativa (P-Value < 0.05)\n",
    "- Eliminar las 2 características numéricas que no son estadísticamente significativas (num_executed_instructions y cpu_usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la lista final de características seleccionadas\n",
    "final_features = [\n",
    "    # Numéricas significativas\n",
    "    'energy_efficiency',\n",
    "    'task_priority',\n",
    "    'network_traffic',\n",
    "    'memory_usage',\n",
    "    'execution_time',\n",
    "    'power_consumption',\n",
    "    # Categóricas (todas fueron significativas)\n",
    "    'task_type_io',\n",
    "    'task_type_network',\n",
    "    'task_status_running',\n",
    "    'task_status_waiting'\n",
    "]\n",
    "\n",
    "# Crear los nuevos DataFrames solo con las características seleccionadas\n",
    "X_train_selected = X_train_resampled[final_features]\n",
    "X_test_selected = X_test[final_features]\n",
    "\n",
    "print(\"Dimensiones del conjunto de entrenamiento después de la selección:\", X_train_selected.shape)\n",
    "print(\"Dimensiones del conjunto de prueba después de la selección:\", X_test_selected.shape)\n",
    "print(\"\\nCaracterísticas finales:\")\n",
    "print(X_train_selected.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebe493",
   "metadata": {},
   "source": [
    "#### Escalado de caracteristicas\n",
    "Después de seleccionar las características, escalaremos sus valores para que todos tengan una distribución con media 0 y desviación estándar 1. Esto es crucial para modelos sensibles a la escala, como la Regresión Logística y las Redes Neuronales, ya que ayuda a que los algoritmos de optimización converjan mucho más rápido y de manera más estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "\n",
    "# 3. Aplicar la MISMA transformación a los datos de prueba\n",
    "X_test_scaled = scaler.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccee7d",
   "metadata": {},
   "source": [
    "## 4. Experimentación y Ajuste de Parámetros\n",
    "En esta fase, entrenaremos y evaluaremos los diferentes modelos de clasificación solicitados en la guía. Para cada modelo,\n",
    "utilizaremos `GridSearchCV` para realizar una búsqueda exhaustiva de los mejores hiperparámetros. `GridSearchCV` utiliza validación\n",
    "cruzada (`cross-validation`) para asegurar que los resultados sean robustos y evitar el sobreajuste.\n",
    "\n",
    "El rendimiento de cada modelo se evaluará inicialmente con la métrica de `accuracy` durante la búsqueda de parámetros, y el mejor\n",
    "modelo de cada tipo se evaluará finalmente sobre el conjunto de prueba (`X_test_selected`) utilizando un reporte de clasificación\n",
    "completo (precisión, recall, f1-score) y una matriz de confusión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446bafd",
   "metadata": {},
   "source": [
    "### Modelo 1:  Regresión Logistica ( Analogo de Regresioń Multivariada)\n",
    "\n",
    "Para nuestro caso particular usaremos este modelo que se utiliza para predecir la probabilidad de un evento binario (como es nuestro caso) basandose en uno o más predictores. Utilizando la función logística `sigmoide` para transformar los resultados en una valor entre 0 y 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed37fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda para Regresión Logística (con datos escalados)...\")\n",
    "grid_search_lr.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "print(\"\\nMejor puntuación de cross-validation (accuracy):\")\n",
    "print(f\"{grid_search_lr.best_score_:.4f}\")\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_pred_lr = best_lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación en el Conjunto de Prueba (con datos escalados) ---\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "ConfusionMatrixDisplay.from_estimator(best_lr, X_test_scaled, y_test, cmap='Blues')\n",
    "plt.title('Matriz de Confusión - Regresión Logística (Datos Escalados)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bd616",
   "metadata": {},
   "source": [
    "### Modelo 2: Árbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ed362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Definir el modelo y los hiperparámetros a probar ---\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# --- 2. Configurar y ejecutar GridSearchCV ---\n",
    "# Usamos los datos escalados, aunque los árboles no son sensibles a la escala\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Árbol de Decisión...\")\n",
    "grid_search_dt.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# --- 3. Mostrar los mejores resultados de la búsqueda ---\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(grid_search_dt.best_params_)\n",
    "print(\"\\nMejor puntuación de cross-validation (accuracy):\")\n",
    "print(f\"{grid_search_dt.best_score_:.4f}\")\n",
    "\n",
    "# --- 4. Evaluar el mejor modelo en el conjunto de prueba ---\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación en el Conjunto de Prueba ---\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# --- 5. Visualizar la Matriz de Confusión ---\n",
    "print(\"Matriz de Confusión:\")\n",
    "ConfusionMatrixDisplay.from_estimator(best_dt, X_test_scaled, y_test, cmap='Greens')\n",
    "plt.title('Matriz de Confusión - Árbol de Decisión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f632f97",
   "metadata": {},
   "source": [
    "## Modelo 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f417899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# --- 2. Configurar y ejecutar GridSearchCV ---\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Random Forest...\")\n",
    "grid_search_rf.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# --- 3. Mostrar los mejores resultados de la búsqueda ---\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "print(\"\\nMejor puntuación de cross-validation (accuracy):\")\n",
    "print(f\"{grid_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# --- 4. Evaluar el mejor modelo en el conjunto de prueba ---\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación en el Conjunto de Prueba ---\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# --- 5. Visualizar la Matriz de Confusión ---\n",
    "print(\"Matriz de Confusión:\")\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test_scaled, y_test, cmap='Purples')\n",
    "plt.title('Matriz de Confusión - Random Forest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b4f83",
   "metadata": {},
   "source": [
    "## Modelo 4: Redes Neuronales (MLP)\n",
    "\n",
    "El modelo que usaremos es el MLPClassifier, que es un perceptron de Scikit-learn, que es un perceptrón Multicapa. Es ideal para este tipo de problemas y muy sensible al escalado de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo y los hiperparámetros a probar\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "# 2. Configurar y ejecutar GridSearchCV\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=42),\n",
    "    param_grid=param_grid_mlp,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Red Neuronal (MLP)...\")\n",
    "grid_search_mlp.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# 3. Mostrar los mejores resultados\n",
    "print(\"\\\\nMejores hiperparámetros encontrados:\")\n",
    "print(grid_search_mlp.best_params_)\n",
    "print(f\"\\\\nMejor puntuación de cross-validation (accuracy): {grid_search_mlp.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# 4. Evaluar el mejor modelo en el conjunto de prueba\n",
    "best_mlp = grid_search_mlp.best_estimator_\n",
    "y_pred_mlp = best_mlp.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\\\n--- Reporte de Clasificación en el Conjunto de Prueba ---\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# 5. Visualizar la Matriz de Confusión\n",
    "print(\"Matriz de Confusión:\")\n",
    "ConfusionMatrixDisplay.from_estimator(best_mlp, X_test_scaled, y_test, cmap='Oranges')\n",
    "plt.title('Matriz de Confusión - Red Neuronal (MLP)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7c673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6542fa99",
   "metadata": {},
   "source": [
    "## 5. Descripción de los experimentos de ajuste de paramentros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23761d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d8a0d37",
   "metadata": {},
   "source": [
    "## 5. Analisis de Resultados Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa944a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df0e9ca",
   "metadata": {},
   "source": [
    "# III. Comparación de Modelos entrenados y conclusiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31a3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95dd7a26",
   "metadata": {},
   "source": [
    "## 6. Comparacion del mejor modelo obtenido por cada método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c9bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c7807fc",
   "metadata": {},
   "source": [
    "## 7. Conclusiones del trabajo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
